# scripts/train_semantic_segmentation.py
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import numpy as np
import time
import os
from pathlib import Path
import sys
from tqdm import tqdm
import datetime

project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from pointnet2_complete import get_complete_model
from pointnet_dataloader import get_data_loaders


class SemanticSegmentationTrainer:
    def __init__(self, model, train_loader, val_loader, device):
        self.model = model.to(device)
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.device = device

        # ä½¿ç”¨CrossEntropyLossï¼Œå¿½ç•¥èƒŒæ™¯ç±»(0)
        self.criterion = nn.CrossEntropyLoss(ignore_index=0)
        self.optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)
        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=20, gamma=0.5)

        self.best_val_loss = float('inf')
        self.best_val_acc = 0.0
        self.train_losses = []
        self.val_losses = []
        self.val_accuracies = []
        self.learning_rates = []

        # è®­ç»ƒç»Ÿè®¡
        self.start_time = None
        self.epoch_times = []

    def train_epoch(self, epoch, total_epochs):
        self.model.train()
        running_loss = 0.0
        total_samples = 0

        # åˆ›å»ºè¿›åº¦æ¡
        pbar = tqdm(total=len(self.train_loader),
                    desc=f'Epoch {epoch + 1}/{total_epochs}',
                    ncols=100,
                    bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]')

        for batch_idx, (points, labels) in enumerate(self.train_loader):
            points = points.to(self.device)
            labels = labels.to(self.device)

            self.optimizer.zero_grad()

            # å‰å‘ä¼ æ’­
            outputs = self.model(points)
            loss = self.criterion(outputs, labels)

            # åå‘ä¼ æ’­
            loss.backward()
            self.optimizer.step()

            running_loss += loss.item() * points.size(0)
            total_samples += points.size(0)

            # æ›´æ–°è¿›åº¦æ¡
            current_loss = running_loss / total_samples
            pbar.set_postfix({
                'loss': f'{current_loss:.4f}',
                'lr': f'{self.optimizer.param_groups[0]["lr"]:.6f}'
            })
            pbar.update(1)

        pbar.close()
        epoch_loss = running_loss / total_samples
        return epoch_loss

    def validate(self, epoch, total_epochs):
        self.model.eval()
        running_loss = 0.0
        total_correct = 0
        total_samples = 0

        # éªŒè¯è¿›åº¦æ¡
        pbar = tqdm(total=len(self.val_loader),
                    desc=f'Validation {epoch + 1}/{total_epochs}',
                    ncols=100,
                    bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]')

        with torch.no_grad():
            for points, labels in self.val_loader:
                points = points.to(self.device)
                labels = labels.to(self.device)

                outputs = self.model(points)
                loss = self.criterion(outputs, labels)

                running_loss += loss.item() * points.size(0)

                # è®¡ç®—å‡†ç¡®ç‡
                pred = outputs.argmax(dim=1)
                mask = labels > 0  # åªè€ƒè™‘è½¦è¾†(1)å’Œè¡Œäºº(2)

                if mask.sum() > 0:
                    correct = (pred[mask] == labels[mask]).sum().item()
                    total_correct += correct
                    total_samples += mask.sum().item()

                pbar.update(1)

        pbar.close()

        val_loss = running_loss / len(self.val_loader.dataset)
        val_acc = total_correct / total_samples if total_samples > 0 else 0

        return val_loss, val_acc

    def print_training_info(self):
        """æ‰“å°è®­ç»ƒä¿¡æ¯"""
        print("\n" + "=" * 80)
        print("ğŸ‹ï¸â€â™‚ï¸ ç‚¹äº‘è¯­ä¹‰åˆ†å‰²è®­ç»ƒå¼€å§‹")
        print("=" * 80)
        print(f"ğŸ“Š è®¾å¤‡: {self.device}")
        print(f"ğŸ“ˆ è®­ç»ƒæ ·æœ¬: {len(self.train_loader.dataset):,}")
        print(f"ğŸ“‰ éªŒè¯æ ·æœ¬: {len(self.val_loader.dataset):,}")
        print(f"ğŸ”¢ Batch size: {self.train_loader.batch_size}")
        print(f"ğŸ“ æ¯æ ·æœ¬ç‚¹æ•°: {self.train_loader.dataset.num_points}")
        print(f"ğŸ”„ æ€»Epochæ•°: {self.epochs}")

        # æ¨¡å‹ä¿¡æ¯
        total_params = sum(p.numel() for p in self.model.parameters())
        print(f"ğŸ§  æ¨¡å‹å‚æ•°: {total_params:,}")

        # æµ‹è¯•ç»´åº¦
        test_points, test_labels = next(iter(self.train_loader))
        test_points = test_points.to(self.device)
        test_outputs = self.model(test_points)
        print(f"ğŸ“ ç»´åº¦æ£€æŸ¥ - è¾“å…¥: {test_points.shape}, è¾“å‡º: {test_outputs.shape}, æ ‡ç­¾: {test_labels.shape}")
        print("=" * 80 + "\n")

    def print_epoch_summary(self, epoch, train_loss, val_loss, val_acc, epoch_time, total_epochs):
        """æ‰“å°epochæ€»ç»“"""
        # è®¡ç®—å‰©ä½™æ—¶é—´
        avg_epoch_time = np.mean(self.epoch_times)
        remaining_epochs = total_epochs - epoch - 1
        remaining_time = avg_epoch_time * remaining_epochs
        remaining_str = str(datetime.timedelta(seconds=int(remaining_time)))

        # è®¡ç®—æ€»è®­ç»ƒæ—¶é—´
        total_time = time.time() - self.start_time
        total_str = str(datetime.timedelta(seconds=int(total_time)))

        print(f"\nğŸ“Š Epoch {epoch + 1}/{total_epochs} æ€»ç»“:")
        print(f"   â±ï¸  æœ¬è½®æ—¶é—´: {epoch_time:.1f}s")
        print(f"   â³ æ€»è®­ç»ƒæ—¶é—´: {total_str}")
        print(f"   ğŸ¯ å‰©ä½™æ—¶é—´: {remaining_str}")
        print(f"   ğŸ“‰ è®­ç»ƒæŸå¤±: {train_loss:.4f}")
        print(f"   ğŸ“Š éªŒè¯æŸå¤±: {val_loss:.4f}")
        print(f"   ğŸ¯ éªŒè¯å‡†ç¡®ç‡: {val_acc:.4f}")
        print(f"   ğŸ“ˆ æœ€ä½³å‡†ç¡®ç‡: {self.best_val_acc:.4f}")
        print(f"   ğŸ”§ å­¦ä¹ ç‡: {self.optimizer.param_groups[0]['lr']:.6f}")

    def train(self, epochs=50, save_dir='checkpoints'):
        self.epochs = epochs
        self.start_time = time.time()
        save_dir = Path(save_dir)
        save_dir.mkdir(exist_ok=True)

        # æ‰“å°è®­ç»ƒä¿¡æ¯
        self.print_training_info()

        for epoch in range(epochs):
            epoch_start_time = time.time()

            # è®­ç»ƒ
            train_loss = self.train_epoch(epoch, epochs)
            self.train_losses.append(train_loss)

            # éªŒè¯
            val_loss, val_acc = self.validate(epoch, epochs)
            self.val_losses.append(val_loss)
            self.val_accuracies.append(val_acc)
            self.learning_rates.append(self.optimizer.param_groups[0]['lr'])

            # å­¦ä¹ ç‡è°ƒåº¦
            self.scheduler.step()

            # è®¡ç®—epochæ—¶é—´
            epoch_time = time.time() - epoch_start_time
            self.epoch_times.append(epoch_time)

            # æ‰“å°epochæ€»ç»“
            self.print_epoch_summary(epoch, train_loss, val_loss, val_acc, epoch_time, epochs)

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_acc > self.best_val_acc:
                self.best_val_acc = val_acc
                self.best_val_loss = val_loss
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': self.model.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'val_loss': val_loss,
                    'val_acc': val_acc,
                    'train_loss': train_loss
                }, save_dir / 'best_model.pth')
                print(f"   ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹! å‡†ç¡®ç‡: {val_acc:.4f}")

            # æ¯10ä¸ªepochä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹
            if (epoch + 1) % 10 == 0:
                checkpoint_path = save_dir / f'checkpoint_epoch_{epoch + 1}.pth'
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': self.model.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'train_loss': train_loss,
                    'val_loss': val_loss,
                    'val_acc': val_acc,
                    'train_losses': self.train_losses,
                    'val_losses': self.val_losses,
                    'val_accuracies': self.val_accuracies
                }, checkpoint_path)
                print(f"   ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: {checkpoint_path}")

            print("-" * 60)

        # è®­ç»ƒå®Œæˆ
        self.print_final_summary()

    def print_final_summary(self):
        """æ‰“å°æœ€ç»ˆæ€»ç»“"""
        total_time = time.time() - self.start_time
        total_str = str(datetime.timedelta(seconds=int(total_time)))

        print("\n" + "=" * 80)
        print("ğŸ‰ è®­ç»ƒå®Œæˆ!")
        print("=" * 80)
        print(f"â±ï¸  æ€»è®­ç»ƒæ—¶é—´: {total_str}")
        print(f"ğŸ“ˆ æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {self.best_val_acc:.4f}")
        print(f"ğŸ“‰ æœ€ä½³éªŒè¯æŸå¤±: {self.best_val_loss:.4f}")
        print(f"ğŸ”„ æ€»Epochæ•°: {self.epochs}")
        print(f"ğŸ“Š æœ€ç»ˆè®­ç»ƒæŸå¤±: {self.train_losses[-1]:.4f}")
        print(f"ğŸ“Š æœ€ç»ˆéªŒè¯æŸå¤±: {self.val_losses[-1]:.4f}")
        print("=" * 80)

        # ä¿å­˜è®­ç»ƒå†å²
        self.save_training_history()

    def save_training_history(self):
        """ä¿å­˜è®­ç»ƒå†å²"""
        history = {
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'val_accuracies': self.val_accuracies,
            'learning_rates': self.learning_rates,
            'epoch_times': self.epoch_times,
            'best_val_acc': self.best_val_acc,
            'best_val_loss': self.best_val_loss
        }

        history_path = Path('checkpoints/training_history.npy')
        np.save(history_path, history)
        print(f"ğŸ“ è®­ç»ƒå†å²å·²ä¿å­˜: {history_path}")


def plot_training_curves(train_losses, val_losses, val_accuracies, learning_rates):
    """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
    try:
        import matplotlib.pyplot as plt

        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))

        # æŸå¤±æ›²çº¿
        epochs = range(1, len(train_losses) + 1)
        ax1.plot(epochs, train_losses, 'b-', label='è®­ç»ƒæŸå¤±', linewidth=2)
        ax1.plot(epochs, val_losses, 'r-', label='éªŒè¯æŸå¤±', linewidth=2)
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        ax1.set_title('è®­ç»ƒå’ŒéªŒè¯æŸå¤±')

        # å‡†ç¡®ç‡æ›²çº¿
        ax2.plot(epochs, val_accuracies, 'g-', label='éªŒè¯å‡†ç¡®ç‡', linewidth=2)
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Accuracy')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        ax2.set_title('éªŒè¯å‡†ç¡®ç‡')
        ax2.set_ylim(0, 1)

        # å­¦ä¹ ç‡æ›²çº¿
        ax3.plot(epochs, learning_rates, 'purple', label='å­¦ä¹ ç‡', linewidth=2)
        ax3.set_xlabel('Epoch')
        ax3.set_ylabel('Learning Rate')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        ax3.set_title('å­¦ä¹ ç‡å˜åŒ–')
        ax3.set_yscale('log')

        # æŸå¤±å’Œå‡†ç¡®ç‡å¯¹æ¯”
        ax4.plot(val_losses, val_accuracies, 'o-', color='orange', linewidth=2)
        ax4.set_xlabel('éªŒè¯æŸå¤±')
        ax4.set_ylabel('éªŒè¯å‡†ç¡®ç‡')
        ax4.grid(True, alpha=0.3)
        ax4.set_title('æŸå¤± vs å‡†ç¡®ç‡')

        plt.tight_layout()
        plt.savefig('training_curves_detailed.png', dpi=300, bbox_inches='tight')
        plt.show()

        print("ğŸ“Š è®­ç»ƒæ›²çº¿å›¾å·²ä¿å­˜: training_curves_detailed.png")

    except ImportError:
        print("âš ï¸  Matplotlibæœªå®‰è£…ï¼Œè·³è¿‡ç»˜å›¾")


def main():
    # å‚æ•°è®¾ç½®
    batch_size = 4
    num_points = 2048
    epochs = 50

    # è®¾å¤‡è®¾ç½®
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # æ•°æ®åŠ è½½å™¨
    train_loader, val_loader, test_loader = get_data_loaders(
        batch_size=batch_size, num_points=num_points
    )

    # ä½¿ç”¨å®Œæ•´çš„PointNet++æ¨¡å‹
    model = get_complete_model(num_classes=3)

    # è®­ç»ƒå™¨
    trainer = SemanticSegmentationTrainer(model, train_loader, val_loader, device)

    # å¼€å§‹è®­ç»ƒ
    train_losses, val_losses, val_accuracies = trainer.train(epochs=epochs)

    # ç»˜åˆ¶è¯¦ç»†çš„è®­ç»ƒæ›²çº¿
    plot_training_curves(train_losses, val_losses, val_accuracies, trainer.learning_rates)


if __name__ == "__main__":
    main()