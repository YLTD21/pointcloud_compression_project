# scripts/train_semantic_segmentation.py
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import numpy as np
import time
import os
from pathlib import Path
import sys
from tqdm import tqdm
import datetime

project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))
try:
    from gpu_optimized_dataloader import get_gpu_optimized_loaders
    GPU_OPTIMIZED_AVAILABLE = True
except ImportError:
    from pointnet_dataloader import get_data_loaders
    GPU_OPTIMIZED_AVAILABLE = False
    print("âš ï¸  GPUä¼˜åŒ–æ•°æ®åŠ è½½å™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨æ™®é€šç‰ˆæœ¬")

from pointnet2_complete import get_complete_model
from pointnet_dataloader import get_data_loaders


class SemanticSegmentationTrainer:
    def __init__(self, model, train_loader, val_loader, device):
        self.model = model.to(device)
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.device = device

        # ä½¿ç”¨CrossEntropyLossï¼Œå¿½ç•¥èƒŒæ™¯ç±»(0)
        self.criterion = nn.CrossEntropyLoss(ignore_index=0)
        self.optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)
        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=20, gamma=0.5)

        self.best_val_loss = float('inf')
        self.best_val_acc = 0.0
        self.train_losses = []
        self.val_losses = []
        self.val_accuracies = []
        self.learning_rates = []

        # è®­ç»ƒç»Ÿè®¡
        self.start_time = None
        self.epoch_times = []
        self.batch_times = []
        self.data_loading_times = []

    def get_detailed_gpu_info(self):
        """è·å–è¯¦ç»†çš„GPUä¿¡æ¯"""
        if not torch.cuda.is_available():
            return "CPUæ¨¡å¼"

        gpu_info = {}
        try:
            # å†…å­˜ä¿¡æ¯
            gpu_info['allocated_gb'] = torch.cuda.memory_allocated() / 1024 ** 3
            gpu_info['reserved_gb'] = torch.cuda.memory_reserved() / 1024 ** 3
            gpu_info['max_allocated_gb'] = torch.cuda.max_memory_allocated() / 1024 ** 3

            # åˆ©ç”¨ç‡å’Œæ¸©åº¦ï¼ˆéœ€è¦nvidia-smiï¼‰
            import subprocess
            result = subprocess.run([
                'nvidia-smi', '--query-gpu=utilization.gpu,temperature.gpu',
                '--format=csv,noheader,nounits'
            ], capture_output=True, text=True)

            if result.returncode == 0:
                util, temp = result.stdout.strip().split(', ')
                gpu_info['utilization'] = f"{util}%"
                gpu_info['temperature'] = f"{temp}Â°C"
            else:
                gpu_info['utilization'] = "N/A"
                gpu_info['temperature'] = "N/A"

        except Exception as e:
            gpu_info['error'] = str(e)

        return gpu_info
    # åœ¨ train_semantic_segmentation.py ä¸­ä¿®æ”¹ä»¥ä¸‹éƒ¨åˆ†

    # åœ¨ train_semantic_segmentation.py ä¸­ä¿®æ”¹ä»¥ä¸‹éƒ¨åˆ†

    def train_epoch(self, epoch, total_epochs):
        self.model.train()
        running_loss = 0.0
        total_samples = 0

        # é‡ç½®GPUå†…å­˜ç»Ÿè®¡
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            initial_memory = torch.cuda.memory_allocated() / 1024 ** 3

        # æ€§èƒ½è®¡æ—¶
        epoch_start_time = time.time()
        data_loading_time = 0
        computation_time = 0

        pbar = tqdm(total=len(self.train_loader),
                    desc=f'Epoch {epoch + 1}/{total_epochs}',
                    ncols=120,
                    bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]')

        # æ··åˆç²¾åº¦è®¾ç½®
        if torch.cuda.is_available():
            scaler = torch.amp.GradScaler('cuda')
            autocast_device = 'cuda'
        else:
            scaler = None
            autocast_device = 'cpu'

        for batch_idx, (points, labels) in enumerate(self.train_loader):
            batch_data_loading_time = time.time()
            data_loading_time += batch_data_loading_time - epoch_start_time

            # æ•°æ®ä¼ è¾“
            points = points.to(self.device, non_blocking=True)
            labels = labels.to(self.device, non_blocking=True)

            self.optimizer.zero_grad(set_to_none=True)

            # å‰å‘ä¼ æ’­
            batch_computation_start = time.time()

            if scaler is not None:
                with torch.amp.autocast(autocast_device):
                    outputs = self.model(points)
                    loss = self.criterion(outputs, labels)

                scaler.scale(loss).backward()
                scaler.step(self.optimizer)
                scaler.update()
            else:
                outputs = self.model(points)
                loss = self.criterion(outputs, labels)
                loss.backward()
                self.optimizer.step()

            computation_time += time.time() - batch_computation_start

            running_loss += loss.item() * points.size(0)
            total_samples += points.size(0)

            # æ›´æ–°è¿›åº¦æ¡
            current_loss = running_loss / total_samples

            # è·å–è¯¦ç»†çš„GPUä¿¡æ¯
            gpu_info = self.get_detailed_gpu_info()
            memory_allocated = gpu_info.get('allocated_gb', 0)

            pbar.set_postfix({
                'loss': f'{current_loss:.4f}',
                'lr': f'{self.optimizer.param_groups[0]["lr"]:.6f}',
                'gpu_mem': f'{memory_allocated:.1f}GB',
                'data_time': f'{data_loading_time / (batch_idx + 1):.3f}s',
                'comp_time': f'{computation_time / (batch_idx + 1):.3f}s'
            })
            pbar.update(1)

            epoch_start_time = time.time()  # é‡ç½®è®¡æ—¶

        pbar.close()

        # æ‰“å°è¯¦ç»†çš„æ€§èƒ½ç»Ÿè®¡
        epoch_time = time.time() - self.start_time if hasattr(self, 'start_time') else 0
        self.batch_times.append(epoch_time)

        if torch.cuda.is_available():
            peak_memory = torch.cuda.max_memory_allocated() / 1024 ** 3
            print(f"ğŸ’¾ GPUå†…å­˜: åˆå§‹ {initial_memory:.1f}GB, å³°å€¼ {peak_memory:.1f}GB")
            print(f"â±ï¸  æ•°æ®åŠ è½½: {data_loading_time:.2f}s, è®¡ç®—: {computation_time:.2f}s")

        epoch_loss = running_loss / total_samples
        return epoch_loss

    def validate(self, epoch, total_epochs):
        self.model.eval()
        running_loss = 0.0
        total_correct = 0
        total_samples = 0
        total_points = 0

        # ä½¿ç”¨æ›´æµç•…çš„è¿›åº¦æ¡
        from alive_progress import alive_bar

        print(f"ğŸ” éªŒè¯ Epoch {epoch + 1}/{total_epochs}")

        with torch.no_grad():
            # ä½¿ç”¨ alive_bar æ›¿ä»£ tqdm
            with alive_bar(len(self.val_loader),
                           title='éªŒè¯è¿›åº¦',
                           bar='smooth',
                           spinner='dots',
                           length=50,
                           stats=False,  # å…³é—­ç»Ÿè®¡ä¿¡æ¯å‡å°‘å¼€é”€
                           monitor=False,  # å…³é—­ç›‘æ§å‡å°‘å¼€é”€
                           elapsed=False,  # å…³é—­è€—æ—¶æ˜¾ç¤º
                           receipt=False) as bar:  # å…³é—­æ”¶æ®æ˜¾ç¤º

                for batch_idx, (points, labels) in enumerate(self.val_loader):
                    # å¼‚æ­¥æ•°æ®ä¼ è¾“
                    points = points.to(self.device, non_blocking=True)
                    labels = labels.to(self.device, non_blocking=True)

                    # ä½¿ç”¨æ··åˆç²¾åº¦åŠ é€Ÿæ¨ç†
                    if torch.cuda.is_available():
                        with torch.amp.autocast('cuda'):
                            outputs = self.model(points)
                            loss = self.criterion(outputs, labels)
                    else:
                        outputs = self.model(points)
                        loss = self.criterion(outputs, labels)

                    running_loss += loss.item() * points.size(0)

                    # è®¡ç®—å‡†ç¡®ç‡ - ä½¿ç”¨å¼‚æ­¥æ“ä½œ
                    pred = outputs.argmax(dim=1)
                    mask = labels > 0  # åªè€ƒè™‘è½¦è¾†(1)å’Œè¡Œäºº(2)

                    if mask.sum() > 0:
                        correct = (pred[mask] == labels[mask]).sum().item()
                        total_correct += correct
                        total_samples += mask.sum().item()

                    total_points += points.size(0)

                    # å®æ—¶æ›´æ–°è¿›åº¦æ¡ä¿¡æ¯
                    current_loss = running_loss / total_points
                    current_acc = total_correct / total_samples if total_samples > 0 else 0

                    # æ¯10ä¸ªbatchæˆ–æœ€åä¸€ä¸ªbatchæ›´æ–°ä¸€æ¬¡æ˜¾ç¤ºï¼Œå‡å°‘å¼€é”€
                    if batch_idx % 10 == 0 or batch_idx == len(self.val_loader) - 1:
                        bar.text = f'æŸå¤±: {current_loss:.4f} | å‡†ç¡®ç‡: {current_acc:.4f}'

                    bar()  # æ›´æ–°è¿›åº¦æ¡

        val_loss = running_loss / len(self.val_loader.dataset)
        val_acc = total_correct / total_samples if total_samples > 0 else 0

        print(f"âœ… éªŒè¯å®Œæˆ: æŸå¤±={val_loss:.4f}, å‡†ç¡®ç‡={val_acc:.4f}")
        return val_loss, val_acc

    def print_training_info(self):
        """æ‰“å°è®­ç»ƒä¿¡æ¯"""
        print("\n" + "=" * 80)
        print("ğŸ‹ï¸â€â™‚ï¸ ç‚¹äº‘è¯­ä¹‰åˆ†å‰²è®­ç»ƒå¼€å§‹ - ä¼˜åŒ–ç‰ˆæœ¬")
        print("=" * 80)
        print(f"ğŸ“Š è®¾å¤‡: {self.device}")
        print(f"ğŸ“ˆ è®­ç»ƒæ ·æœ¬: {len(self.train_loader.dataset):,}")
        print(f"ğŸ“‰ éªŒè¯æ ·æœ¬: {len(self.val_loader.dataset):,}")
        print(f"ğŸ”¢ Batch size: {self.train_loader.batch_size}")
        print(f"ğŸ“ æ¯æ ·æœ¬ç‚¹æ•°: {self.train_loader.dataset.num_points}")
        print(f"ğŸ”„ æ€»Epochæ•°: {self.epochs}")

        # æ•°æ®åŠ è½½å™¨ä¿¡æ¯
        dataset_info = self.train_loader.dataset.get_dataset_info()
        print(f"ğŸ’¾ æ•°æ®é¢„åŠ è½½: {dataset_info['preloaded']}")
        print(f"ğŸ“¦ ç¼“å­˜æ ·æœ¬: {dataset_info['cached_samples']}")

        # æ¨¡å‹ä¿¡æ¯
        total_params = sum(p.numel() for p in self.model.parameters())
        print(f"ğŸ§  æ¨¡å‹å‚æ•°: {total_params:,}")

        # GPUä¿¡æ¯
        if torch.cuda.is_available():
            gpu_info = self.get_detailed_gpu_info()
            print(f"ğŸ® GPUå†…å­˜: {gpu_info.get('allocated_gb', 0):.1f}GB / {gpu_info.get('reserved_gb', 0):.1f}GB")
            print(f"ğŸ”¥ GPUåˆ©ç”¨ç‡: {gpu_info.get('utilization', 'N/A')}")

        print("=" * 80 + "\n")

    def print_epoch_summary(self, epoch, train_loss, val_loss, val_acc, epoch_time, total_epochs):
        """æ‰“å°epochæ€»ç»“"""
        # è®¡ç®—å‰©ä½™æ—¶é—´
        avg_epoch_time = np.mean(self.epoch_times)
        remaining_epochs = total_epochs - epoch - 1
        remaining_time = avg_epoch_time * remaining_epochs
        remaining_str = str(datetime.timedelta(seconds=int(remaining_time)))

        # è®¡ç®—æ€»è®­ç»ƒæ—¶é—´
        total_time = time.time() - self.start_time
        total_str = str(datetime.timedelta(seconds=int(total_time)))

        print(f"\nğŸ“Š Epoch {epoch + 1}/{total_epochs} æ€»ç»“:")
        print(f"   â±ï¸  æœ¬è½®æ—¶é—´: {epoch_time:.1f}s")
        print(f"   â³ æ€»è®­ç»ƒæ—¶é—´: {total_str}")
        print(f"   ğŸ¯ å‰©ä½™æ—¶é—´: {remaining_str}")
        print(f"   ğŸ“‰ è®­ç»ƒæŸå¤±: {train_loss:.4f}")
        print(f"   ğŸ“Š éªŒè¯æŸå¤±: {val_loss:.4f}")
        print(f"   ğŸ¯ éªŒè¯å‡†ç¡®ç‡: {val_acc:.4f}")
        print(f"   ğŸ“ˆ æœ€ä½³å‡†ç¡®ç‡: {self.best_val_acc:.4f}")
        print(f"   ğŸ”§ å­¦ä¹ ç‡: {self.optimizer.param_groups[0]['lr']:.6f}")

    def train(self, epochs=50, save_dir='checkpoints'):
        self.epochs = epochs
        self.start_time = time.time()
        save_dir = Path(save_dir)
        save_dir.mkdir(exist_ok=True)

        # æ‰“å°è®­ç»ƒä¿¡æ¯
        self.print_training_info()

        for epoch in range(epochs):
            epoch_start_time = time.time()

            # è®­ç»ƒ
            train_loss = self.train_epoch(epoch, epochs)
            self.train_losses.append(train_loss)

            # éªŒè¯
            val_loss, val_acc = self.validate(epoch, epochs)
            self.val_losses.append(val_loss)
            self.val_accuracies.append(val_acc)
            self.learning_rates.append(self.optimizer.param_groups[0]['lr'])

            # å­¦ä¹ ç‡è°ƒåº¦
            self.scheduler.step()

            # è®¡ç®—epochæ—¶é—´
            epoch_time = time.time() - epoch_start_time
            self.epoch_times.append(epoch_time)

            # æ‰“å°epochæ€»ç»“
            self.print_epoch_summary(epoch, train_loss, val_loss, val_acc, epoch_time, epochs)

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_acc > self.best_val_acc:
                self.best_val_acc = val_acc
                self.best_val_loss = val_loss
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': self.model.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'val_loss': val_loss,
                    'val_acc': val_acc,
                    'train_loss': train_loss
                }, save_dir / 'best_model.pth')
                print(f"   ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹! å‡†ç¡®ç‡: {val_acc:.4f}")

            # æ¯10ä¸ªepochä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹
            if (epoch + 1) % 10 == 0:
                checkpoint_path = save_dir / f'checkpoint_epoch_{epoch + 1}.pth'
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': self.model.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'train_loss': train_loss,
                    'val_loss': val_loss,
                    'val_acc': val_acc,
                    'train_losses': self.train_losses,
                    'val_losses': self.val_losses,
                    'val_accuracies': self.val_accuracies
                }, checkpoint_path)
                print(f"   ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: {checkpoint_path}")

            print("-" * 60)

        # è®­ç»ƒå®Œæˆ
        self.print_final_summary()

    def print_final_summary(self):
        """æ‰“å°æœ€ç»ˆæ€»ç»“"""
        total_time = time.time() - self.start_time
        total_str = str(datetime.timedelta(seconds=int(total_time)))

        print("\n" + "=" * 80)
        print("ğŸ‰ è®­ç»ƒå®Œæˆ!")
        print("=" * 80)
        print(f"â±ï¸  æ€»è®­ç»ƒæ—¶é—´: {total_str}")
        print(f"ğŸ“ˆ æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {self.best_val_acc:.4f}")
        print(f"ğŸ“‰ æœ€ä½³éªŒè¯æŸå¤±: {self.best_val_loss:.4f}")
        print(f"ğŸ”„ æ€»Epochæ•°: {self.epochs}")
        print(f"ğŸ“Š æœ€ç»ˆè®­ç»ƒæŸå¤±: {self.train_losses[-1]:.4f}")
        print(f"ğŸ“Š æœ€ç»ˆéªŒè¯æŸå¤±: {self.val_losses[-1]:.4f}")
        print("=" * 80)

        # ä¿å­˜è®­ç»ƒå†å²
        self.save_training_history()

    def save_training_history(self):
        """ä¿å­˜è®­ç»ƒå†å²"""
        history = {
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'val_accuracies': self.val_accuracies,
            'learning_rates': self.learning_rates,
            'epoch_times': self.epoch_times,
            'best_val_acc': self.best_val_acc,
            'best_val_loss': self.best_val_loss
        }

        history_path = Path('checkpoints/training_history.npy')
        np.save(history_path, history)
        print(f"ğŸ“ è®­ç»ƒå†å²å·²ä¿å­˜: {history_path}")


def plot_training_curves(train_losses, val_losses, val_accuracies, learning_rates):
    """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
    try:
        import matplotlib.pyplot as plt

        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))

        # æŸå¤±æ›²çº¿
        epochs = range(1, len(train_losses) + 1)
        ax1.plot(epochs, train_losses, 'b-', label='è®­ç»ƒæŸå¤±', linewidth=2)
        ax1.plot(epochs, val_losses, 'r-', label='éªŒè¯æŸå¤±', linewidth=2)
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        ax1.set_title('è®­ç»ƒå’ŒéªŒè¯æŸå¤±')

        # å‡†ç¡®ç‡æ›²çº¿
        ax2.plot(epochs, val_accuracies, 'g-', label='éªŒè¯å‡†ç¡®ç‡', linewidth=2)
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Accuracy')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        ax2.set_title('éªŒè¯å‡†ç¡®ç‡')
        ax2.set_ylim(0, 1)

        # å­¦ä¹ ç‡æ›²çº¿
        ax3.plot(epochs, learning_rates, 'purple', label='å­¦ä¹ ç‡', linewidth=2)
        ax3.set_xlabel('Epoch')
        ax3.set_ylabel('Learning Rate')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        ax3.set_title('å­¦ä¹ ç‡å˜åŒ–')
        ax3.set_yscale('log')

        # æŸå¤±å’Œå‡†ç¡®ç‡å¯¹æ¯”
        ax4.plot(val_losses, val_accuracies, 'o-', color='orange', linewidth=2)
        ax4.set_xlabel('éªŒè¯æŸå¤±')
        ax4.set_ylabel('éªŒè¯å‡†ç¡®ç‡')
        ax4.grid(True, alpha=0.3)
        ax4.set_title('æŸå¤± vs å‡†ç¡®ç‡')

        plt.tight_layout()
        plt.savefig('training_curves_detailed.png', dpi=300, bbox_inches='tight')
        plt.show()

        print("ğŸ“Š è®­ç»ƒæ›²çº¿å›¾å·²ä¿å­˜: training_curves_detailed.png")

    except ImportError:
        print("âš ï¸  Matplotlibæœªå®‰è£…ï¼Œè·³è¿‡ç»˜å›¾")


def main():
    # å¤§å¹…å¢åŠ batch_sizeæ¥å……åˆ†åˆ©ç”¨GPUï¼
    batch_size = 64  # ä»4å¢åŠ åˆ°64ï¼Œç”šè‡³128
    num_points = 4096
    epochs = 50

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ¯ ä½¿ç”¨è®¾å¤‡: {device}")

    # é€‰æ‹©æ•°æ®åŠ è½½å™¨
    if GPU_OPTIMIZED_AVAILABLE and torch.cuda.is_available():
        print("ğŸš€ ä½¿ç”¨GPUä¼˜åŒ–æ•°æ®åŠ è½½å™¨")
        train_loader, val_loader, test_loader = get_gpu_optimized_loaders(
            batch_size=batch_size,
            num_points=num_points
        )
    else:
        print("âš¡ ä½¿ç”¨æ™®é€šæ•°æ®åŠ è½½å™¨")
        train_loader, val_loader, test_loader = get_data_loaders(
            batch_size=batch_size,
            num_points=num_points,
            preload_to_ram=False  # å¦‚æœå†…å­˜ä¸è¶³
        )

    # æµ‹è¯•æ•°æ®åŠ è½½æ€§èƒ½
    test_data_loading_performance(train_loader)

    # ä½¿ç”¨å®Œæ•´çš„PointNet++æ¨¡å‹
    model = get_complete_model(num_classes=3)

    # è®­ç»ƒå™¨
    trainer = SemanticSegmentationTrainer(model, train_loader, val_loader, device)

    # å¼€å§‹è®­ç»ƒ
    trainer.train(epochs=epochs)

    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    plot_training_curves(trainer.train_losses, trainer.val_losses,
                         trainer.val_accuracies, trainer.learning_rates)


def test_data_loading_performance(train_loader):
    """æµ‹è¯•æ•°æ®åŠ è½½æ€§èƒ½"""
    print("\nğŸ§ª æµ‹è¯•æ•°æ®åŠ è½½æ€§èƒ½...")
    import time

    # é¢„çƒ­
    for i, batch in enumerate(train_loader):
        if i == 2:
            break

    # æ­£å¼æµ‹è¯•
    start_time = time.time()
    batch_count = 0

    for i, (points, labels) in enumerate(train_loader):
        batch_count += 1
        if i == 10:  # æµ‹è¯•10ä¸ªbatch
            break

    total_time = time.time() - start_time
    avg_batch_time = total_time / batch_count

    print(f"ğŸ“Š æ•°æ®åŠ è½½æ€§èƒ½:")
    print(f"   - Batchå¤§å°: {train_loader.batch_size}")
    print(f"   - å¹³å‡æ¯ä¸ªbatch: {avg_batch_time:.4f}s")
    print(f"   - é¢„è®¡è®­ç»ƒé€Ÿåº¦: {train_loader.batch_size / avg_batch_time:.1f} æ ·æœ¬/ç§’")
    print(f"   - GPUåˆ©ç”¨ç‡é¢„æµ‹: {'é«˜' if avg_batch_time < 0.1 else 'ä¸­' if avg_batch_time < 0.5 else 'ä½'}")
if __name__ == "__main__":
    main()