# scripts/gpu_optimized_dataloader.py
import os
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from pathlib import Path
import sys
from tqdm import tqdm

project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))


class GPUDirectDataset(Dataset):
    """ç›´æ¥å°†æ•°æ®é¢„åŠ è½½åˆ°GPUçš„æ•°æ®é›†"""

    def __init__(self, sequences, data_dir, num_points=4096, device='cuda'):
        self.device = device
        self.num_points = num_points

        # è¯­ä¹‰æ ‡ç­¾æ˜ å°„
        self.learning_map = {
            0: 0, 1: 0, 10: 1, 11: 1, 13: 1, 15: 1, 16: 1, 18: 1, 20: 1,
            30: 1, 31: 2, 32: 2, 252: 1, 253: 2, 254: 2, 255: 2, 256: 1,
            257: 1, 258: 1, 259: 1
        }

        self.points_tensors = []
        self.labels_tensors = []

        self._preload_to_gpu(sequences, data_dir)

    def _preload_to_gpu(self, sequences, data_dir):
        """ç›´æ¥å°†æ•°æ®é¢„åŠ è½½åˆ°GPU"""
        print("ğŸš€ ç›´æ¥å°†æ•°æ®åŠ è½½åˆ°GPUå†…å­˜...")

        total_files = 0
        for seq in sequences:
            velodyne_path = data_dir / "dataset" / "sequences" / seq / "velodyne"
            labels_path = data_dir / "dataset" / "sequences" / seq / "labels"

            if not velodyne_path.exists() or not labels_path.exists():
                continue

            bin_files = sorted(velodyne_path.glob("*.bin"))
            label_files = sorted(labels_path.glob("*.label"))
            total_files += len(bin_files)

        print(f"ğŸ“ æ‰¾åˆ° {total_files} ä¸ªæ ·æœ¬ï¼Œå¼€å§‹GPUé¢„åŠ è½½...")

        loaded_count = 0
        for seq in sequences:
            velodyne_path = data_dir / "dataset" / "sequences" / seq / "velodyne"
            labels_path = data_dir / "dataset" / "sequences" / seq / "labels"

            if not velodyne_path.exists():
                continue

            bin_files = sorted(velodyne_path.glob("*.bin"))
            label_files = sorted(labels_path.glob("*.label"))

            for bin_file, label_file in tqdm(zip(bin_files, label_files),
                                             desc=f"åŠ è½½åºåˆ— {seq}",
                                             total=len(bin_files)):
                try:
                    # åŠ è½½æ•°æ®
                    points = np.fromfile(bin_file, dtype=np.float32).reshape(-1, 4)[:, :3]
                    labels = np.fromfile(label_file, dtype=np.uint32) & 0xFFFF

                    # CPUç«¯æœ€å°åŒ–é¢„å¤„ç†
                    points, labels = self._preprocess_on_cpu(points, labels)

                    # ç«‹å³é€åˆ°GPU
                    points_tensor = torch.from_numpy(points).float().to(self.device)
                    labels_tensor = torch.from_numpy(labels).long().to(self.device)

                    self.points_tensors.append(points_tensor)
                    self.labels_tensors.append(labels_tensor)
                    loaded_count += 1

                except Exception as e:
                    print(f"âŒ åŠ è½½å¤±è´¥ {bin_file}: {e}")
                    continue

        print(f"âœ… GPUé¢„åŠ è½½å®Œæˆ: {loaded_count}/{total_files} ä¸ªæ ·æœ¬")
        print(f"ğŸ’¾ GPUå†…å­˜å ç”¨: {self._get_gpu_memory_usage()}")

    def _preprocess_on_cpu(self, points, labels):
        """æœ€å°åŒ–çš„CPUé¢„å¤„ç†"""
        n_points = len(points)

        # ä¸‹é‡‡æ ·
        if n_points >= self.num_points:
            indices = np.random.choice(n_points, self.num_points, replace=False)
        else:
            indices = np.random.choice(n_points, self.num_points, replace=True)

        points = points[indices]
        labels = labels[indices]

        # æ ‡ç­¾æ˜ å°„
        mapped_labels = np.zeros_like(labels, dtype=np.long)
        for original_label, mapped_label in self.learning_map.items():
            mask = labels == original_label
            mapped_labels[mask] = mapped_label
        labels = mapped_labels

        # å½’ä¸€åŒ–
        centroid = np.mean(points, axis=0)
        points = points - centroid
        max_dist = np.max(np.sqrt(np.sum(points ** 2, axis=1)))
        if max_dist > 0:
            points = points / max_dist

        return points, labels

    def _get_gpu_memory_usage(self):
        """è·å–GPUå†…å­˜ä½¿ç”¨æƒ…å†µ"""
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / 1024 ** 3
            return f"{allocated:.1f}GB"
        return "N/A"

    def __getitem__(self, idx):
        # ç›´æ¥ä»GPUå†…å­˜è¿”å›ï¼Œé›¶æ‹·è´ï¼
        return self.points_tensors[idx], self.labels_tensors[idx]

    def __len__(self):
        return len(self.points_tensors)


def get_gpu_optimized_loaders(batch_size=64, num_points=4096):
    """GPUä¼˜åŒ–çš„æ•°æ®åŠ è½½å™¨"""

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    data_dir = project_root / "data" / "raw_dataset"

    print(f"ğŸš€ åˆ›å»ºGPUä¼˜åŒ–æ•°æ®åŠ è½½å™¨")
    print(f"   - è®¾å¤‡: {device}")
    print(f"   - Batch Size: {batch_size}")
    print(f"   - æ•°æ®ä½ç½®: GPUå†…å­˜")

    # æ•°æ®é›†åˆ’åˆ†
    train_sequences = ['00', '01', '02', '03']
    val_sequences = ['04']
    test_sequences = ['05']

    # è®­ç»ƒé›†ä½¿ç”¨GPUé¢„åŠ è½½
    print("ğŸ“¥ åŠ è½½è®­ç»ƒé›†åˆ°GPU...")
    train_dataset = GPUDirectDataset(
        train_sequences, data_dir, num_points, device=device
    )

    # éªŒè¯é›†å’Œæµ‹è¯•é›†å¯ä»¥ä¿æŒåŸæ ·æˆ–ä¹Ÿç”¨GPUåŠ è½½
    print("ğŸ“¥ åŠ è½½éªŒè¯é›†åˆ°GPU...")
    val_dataset = GPUDirectDataset(
        val_sequences, data_dir, num_points, device=device
    )

    print("ğŸ“¥ åŠ è½½æµ‹è¯•é›†åˆ°GPU...")
    test_dataset = GPUDirectDataset(
        test_sequences, data_dir, num_points, device=device
    )

    # æç®€æ•°æ®åŠ è½½å™¨é…ç½®
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=0,  # ä¸éœ€è¦worker
        pin_memory=False,  # ä¸éœ€è¦pin_memory
        drop_last=True
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=0,
        pin_memory=False
    )

    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=0,
        pin_memory=False
    )

    print("âœ… GPUä¼˜åŒ–æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ")
    return train_loader, val_loader, test_loader


# æµ‹è¯•å‡½æ•°
def test_gpu_loading_speed():
    """æµ‹è¯•GPUæ•°æ®åŠ è½½é€Ÿåº¦"""
    print("ğŸ§ª æµ‹è¯•GPUæ•°æ®åŠ è½½é€Ÿåº¦...")

    import time
    train_loader, _, _ = get_gpu_optimized_loaders(batch_size=64, num_points=4096)

    start_time = time.time()
    batch_count = 0

    for i, (points, labels) in enumerate(train_loader):
        batch_count += 1
        if i == 10:  # æµ‹è¯•10ä¸ªbatch
            break

    total_time = time.time() - start_time
    avg_batch_time = total_time / batch_count

    print(f"ğŸ“Š GPUåŠ è½½æ€§èƒ½:")
    print(f"   - å¹³å‡æ¯ä¸ªbatch: {avg_batch_time:.4f}s")
    print(f"   - é¢„è®¡è®­ç»ƒé€Ÿåº¦: {64 / avg_batch_time:.1f} æ ·æœ¬/ç§’")
    print(f"   - CPU-GPUä¼ è¾“: é›¶æ‹·è´")


if __name__ == "__main__":
    test_gpu_loading_speed()